# -*- coding: utf-8 -*-
"""Sentiment Based

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T_kbXyICVW1fBjw_N4vzD-2yoncDFX6o
"""

from google.colab import drive
drive.mount('/content/drive')

"""##Problem Statement
The e-commerce business is quite popular today. Here, you do not need to take orders by going to each customer. A company launches its website to sell the items to the end consumer, and customers can order the products that they require from the same website. Famous examples of such e-commerce companies are Amazon, Flipkart, Myntra, Paytm and Snapdeal.

Suppose you are working as a Machine Learning Engineer in an e-commerce company named 'Ebuss'. Ebuss has captured a huge market share in many fields, and it sells the products in various categories such as household essentials, books, personal care products, medicines, cosmetic items, beauty products, electrical appliances, kitchen and dining products and health care products.

With the advancement in technology, it is imperative for Ebuss to grow quickly in the e-commerce market to become a major leader in the market because it has to compete with the likes of Amazon, Flipkart, etc., which are already market leaders.

As a senior ML Engineer, you are asked to build a model that will improve the recommendations given to the users given their past reviews and ratings.

In order to do this, you planned to build a sentiment-based product recommendation system, which includes the following tasks.

Data sourcing and sentiment analysis Building a recommendation system Improving the recommendations using the sentiment analysis model Deploying the end-to-end project with a user interface
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing necessary libraries

# Numpy & Pandas
import pandas as pd
import numpy as np

from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

import string
import re, nltk, spacy, string
import en_core_web_sm
nlp = en_core_web_sm.load()

# NLTK libraries
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import stopwords
from nltk import FreqDist
from nltk.tokenize import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import wordnet

# Libraries required to plot

import matplotlib.pyplot as plt
# %matplotlib inline
import matplotlib as mpl
import seaborn as sns

#Ignore warnings
import warnings
warnings.filterwarnings('ignore')

# To see the hidden columns in dataframe
pd.set_option('display.max_columns', 200)
pd.set_option('display.max_colwidth', 300)
pd.set_option("display.precision", 2)

# Importing the dataset
df = pd.read_csv('/content/drive/MyDrive/Sentiment Based/Senti.csv')

#Having a look at the dataset
df.head()

#To know the no of rows and columns in a dataset
df.shape

#To know about column names
df.columns

#To get information about the dataset
df.info()

#to see null values
df.isna().sum()

# summing up the missing values (column-wise)
round(100*(df.isnull().sum()/len(df.index)), 2)

#removing the reviews_userCity , reviews_userProvince column as it contains 93% & 99% missing values
df = df.drop(['reviews_userCity','reviews_userProvince'],axis=1)
df.columns

# summing up the missing values (column-wise)
round(100*(df.isnull().sum()/len(df.index)), 2)

df.info()

# summing it up to check how many rows have all missing values
df.isnull().all(axis=1).sum()

# finding sum of misisng values in each row
df.isnull().sum(axis=1)

"""### To know the mode of the below columns and replace with nan values"""

mode1 = df['reviews_didPurchase'].mode()
print(mode1)

mode2 = df['reviews_doRecommend'].mode()
print(mode2)

mode3 = df['manufacturer'].mode()
print(mode3)

mode4 = df['reviews_title'].mode()
print(mode4)

mode5 = df['reviews_username'].mode()
print(mode5)

mode6 = df['reviews_date'].mode()
print(mode6)

df['reviews_didPurchase'] = df['reviews_didPurchase'].replace(to_replace = np.nan, value = 'False')

df['reviews_doRecommend'] = df['reviews_doRecommend'].replace(to_replace = np.nan, value = 'True')

df['manufacturer'] = df['manufacturer'].replace(to_replace = np.nan, value = 'Clorox')

df['reviews_title'] = df['reviews_title'].replace(to_replace = np.nan, value = 'Great Product')

df['reviews_username'] = df['reviews_username'].replace(to_replace = np.nan, value = 'Unknown')

df['reviews_date'] = df['reviews_date'].replace(to_replace = np.nan, value = '2012-01-26T00:00:00.000Z')

# summing up the missing values (column-wise)
round(100*(df.isnull().sum()/len(df.index)), 2)

# Remove the missing row of user_sentiment
df = df[~df.user_sentiment.isnull()]

df.isna().sum() #to see null values

df.describe()

df['reviews_rating'].plot.hist(stacked=True, bins=20, fontsize=12, figsize=(10, 8))

#Visualize using count plot
sns.countplot(x='user_sentiment', data= df, palette="Set2")

#visualize the user_rating
sns.countplot(x='reviews_rating', data= df, palette="Set1")

df.head()

df['user_sentiment'] = pd.get_dummies(df.user_sentiment, drop_first=True)

df.head()

#plot the customers by 'positive user sentiment'
df[df['user_sentiment']==1].groupby('reviews_username')['reviews_username'].count().sort_values(ascending=False)[:10].plot(kind='bar', color='b')

#plot the customers by 'negative user sentiment'
df[df['user_sentiment']==0].groupby('reviews_username')['reviews_username'].count().sort_values(ascending=False)[:10].plot(kind='bar', color='r')

#plot the customers by 'positive user sentiment'
df[df['user_sentiment']==1].groupby('brand')['reviews_username'].count().sort_values(ascending=False)[:10].plot(kind='bar', color='y')

#plot the customers by 'negative user sentiment'
df[df['user_sentiment']==0].groupby('brand')['reviews_username'].count().sort_values(ascending=False)[:10].plot(kind='bar', color='r')

"""## Text Processing"""

# Write your function here to clean the text and remove all the unnecessary elements.

def data_cleaning(column):
    # convert the text to lowercase
    df[column] = df[column].apply(lambda x : x.lower())     
    
    # removing text which is present in square brackets using the regex
    df[column] = df[column].apply(lambda x : re.sub('\[(.*?)\]', "", x))
    
    # removing all the punctuations from the text using the regex
    p = "[" + re.escape(string.punctuation) + "]"
    df[column] = df[column].apply(lambda x : re.sub(p, "", x))
    
    # removing words which contains digits/numbers
    df[column] = df[column].apply(lambda x : re.sub('\S*\d+\S*', "", x))

# Print a sentence 
print(df['reviews_text'][1])

# Apply the above function on the reviews_text column
data_cleaning('reviews_text')

# print the same sentence after cleaning it 
df['reviews_text'][1]

df_clean = df[['id','name', 'reviews_text', 'user_sentiment']]

# This is a helper function to map NTLK position tags
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

#To remove stopwords
stop_words = set(stopwords.words('english'))

def remove_stopword(text):
    words = [word for word in text.split() if word.isalpha() and word not in stop_words]
    return " ".join(words)

#Write your function to Lemmatize the texts

lemmatizer = WordNetLemmatizer()
# Lemmatize the sentence
def lemmatize(text):
    word_pos_tags = nltk.pos_tag(word_tokenize(remove_stopword(text))) # Get position tags
    # Map the position tag and lemmatize the word/token
    words =[lemmatizer.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] 
    return " ".join(words)

import nltk
nltk.download('omw-1.4')
import nltk
nltk.download('averaged_perceptron_tagger')

# apply the above created function on the reviews_text column 

df_clean["reviews_text_cleaned"] = df_clean.reviews_text.apply(lambda x: lemmatize(x))

df_clean.head()

# installing wordcloud
!pip install wordcloud

#Using a word cloud find the top 40 words by frequency among all the articles after processing the text
from wordcloud import WordCloud
#list of stop words
stoplist = set(stopwords.words("english"))

# create the word cloud object
wrdcld = WordCloud(stopwords=stoplist,max_words=40, background_color='lemonchiffon').generate(str(df_clean.reviews_text_cleaned))

print(wrdcld)
# plot the wordcloud object
fig = plt.figure(1)
plt.imshow(wrdcld)
plt.axis('off')
plt.show();

# Write your code here to visualise the data according to the 'Review text' character length
plt.figure(figsize=(10,6))
reviews_new_text = [len(d) for d in df_clean.reviews_text_cleaned]
plt.hist(reviews_new_text, edgecolor='black', bins = 50)
sns.despine()
plt.show()

def getMostCommonWords(reviews, n_most_common):
    # break review column into a list of words, and set each to lowercase
    
    new_reviews = [word for review in reviews for word in \
                         review.lower().split()]


    # remove punctuation from reviews
    new_reviews = [''.join(char for char in review if \
                                 char not in string.punctuation) for \
                         review in new_reviews]


    # remove any empty strings that were created by this process
    new_reviews = [review for review in new_reviews if review]

    return Counter(new_reviews).most_common(n_most_common)

pos_reviews = df_clean[df_clean['user_sentiment']==1]
getMostCommonWords(pos_reviews['reviews_text_cleaned'],10)

neg_reviews = df_clean[df_clean['user_sentiment']==0]
getMostCommonWords(neg_reviews['reviews_text_cleaned'],10)

#Write your code here to find the top 30 unigram frequency in the cleaned datafram(df_clean). 

# Use the CountVectorizer function 
def get_unigrams(docs, n):
    vector = CountVectorizer(ngram_range=(1, 1), stop_words='english').fit(docs)
    bow = vector.transform(docs)
    
    # frequency of each unigram is calculated using the BagOfWords model
    sum_words = bow.sum(axis=0) 
    words_freq = [(word, sum_words[0, i]) for word, i in vector.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

# get the list of unigrams by calling the above defined function
unigrams = get_unigrams(df_clean['reviews_text_cleaned'], 30)

#Print the top 10 words in the unigram frequency
unigrams_df = pd.DataFrame(unigrams, columns=['word', 'count'])
unigrams_df.head(10)

plt.figure(figsize=[50,15])
fig = sns.barplot(x=unigrams_df['word'], y=unigrams_df['count'])

#Write your code here to find the top 30 bigram frequency in the cleaned datafram(df_clean). 

# Use the CountVectorizer function to get the bigram words
def get_bigrams(docs, n):
    vector = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(docs)
    bow = vector.transform(docs)
    
    # frequency of each bigram is calculated using the BagOfWords model
    sum_words = bow.sum(axis=0) 
    words_freq = [(word, sum_words[0, i]) for word, i in vector.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

# get the list of bigram words and frequency by calling the above func
bigrams = get_bigrams(df_clean['reviews_text_cleaned'], 30)

#Print the top 10 words in the bigram frequency
bigrams_df = pd.DataFrame(bigrams, columns=['word', 'count'])
bigrams_df.head(10)

plt.figure(figsize=[50,15])
fig = sns.barplot(x=bigrams_df['word'], y=bigrams_df['count'])

#Write your code here to find the top 30 trigram frequency in the cleaned datafram(df_clean). 

# Use the CountVectorizer function to get the trigram words
def get_trigrams(docs, n):
    vector = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(docs)
    bow = vector.transform(docs)
    
    # frequency of each trigram is calculated using the BagOfWords model
    sum_words = bow.sum(axis=0) 
    words_freq = [(word, sum_words[0, i]) for word, i in vector.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

# get the list of trigram words and frequency by calling the above func
trigrams = get_trigrams(df_clean['reviews_text_cleaned'], 30)

#Print the top 10 words in the trigram frequency

trigrams_df = pd.DataFrame(trigrams, columns=['word', 'count'])
trigrams_df.head(10)

plt.figure(figsize=[50,15])
fig = sns.barplot(x=trigrams_df['word'], y=trigrams_df['count'])

"""## Feature Extraction
Convert the raw texts to a matrix of TF-IDF features

**max_df** is used for removing terms that appear too frequently, also known as "corpus-specific stop words"
max_df = 0.95 means "ignore terms that appear in more than 95% "

**min_df** is used for removing terms that appear too infrequently
min_df = 2 means "ignore terms that appear in less than 2 "

"""

#Write your code here to initialise the TfidfVectorizer 

tfidf_vect = TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)

tf_fit = tfidf_vect.fit(df_clean.reviews_text_cleaned)

"""#### Create a document term matrix using fit_transform

"""

#Write your code here to create the Document Term Matrix by transforming the reviews_text_cleaned column present in df_clean.
dtm = tf_fit.transform(df_clean.reviews_text_cleaned)

# get the list of features 
features = list(tf_fit.get_feature_names())
len(features)

"""## Topic Modelling using NMF

Non-Negative Matrix Factorization (NMF) is an unsupervised technique so there are no labeling of topics that the model will be trained on. The way it works is that, NMF decomposes (or factorizes) high-dimensional vectors into a lower-dimensional representation. These lower-dimensional vectors are non-negative which also means their coefficients are non-negative.

In this task you have to perform the following:

* Find the best number of clusters 
* Apply the best number to create word clusters
* Inspect & validate the correction of each cluster  
* Correct the labels if needed 
* Map the clusters to topics/cluster names
"""

from sklearn.decomposition import NMF

"""## Manual Topic Modeling
You need to do take the trial & error approach to find the best num of topics for your NMF model.

The only parameter that is required is the number of components i.e. the number of topics we want. This is the most crucial step in the whole topic modeling process and will greatly affect how good your final topics are.
"""

#Load your nmf_model with the n_components i.e 5
num_topics = 5 

#keep the random_state =40
nmf_model = NMF(n_components = num_topics, random_state = 40)

W = nmf_model.fit_transform(dtm)
H = nmf_model.components_

#Print the Top15 words for each of the topics

# get the feature names
words = np.array(tf_fit.get_feature_names())

# create a dataframe with featues and the topic-term matrix H
topic_words = pd.DataFrame(np.zeros((num_topics, 15)), index=[f'Topic {i}' for i in range(num_topics)],
                           columns=[f'Word {i}' for i in range(15)]).astype(str)
for i in range(num_topics):
    ix = H[i].argsort()[::-1][:15]
    topic_words.iloc[i] = words[ix]

topic_words

#  Dataframe form of Doc-Topic matrix 
wdf = pd.DataFrame(W, columns=[f'Topic {i}' for i in range(num_topics) ])
wdf.head()

#Assign the best topic 

df_clean['Topic'] = W.argmax(axis=1)

df_clean.head()

df_clean_s = df_clean.groupby('Topic').head(5)
df_clean_s.sort_values('Topic')

"""#### After evaluating the mapping, if the topics assigned are correct then assign these names to the relevant topic:
* Promotion related reviews
* Good reviews
* Dissapointed reviews
* loved product
* great product
"""

#Create the dictionary of Topic names and Topics

Topic_names = { 0 : 'Promotion related reviews',
                1 : 'Good reviews' ,
                2 : 'Dissapointed reviews',
                3 : 'loved product' , 
                4 : 'great product'}
#Replace Topics with Topic Names
df_clean['Topic'] = df_clean['Topic'].map(Topic_names)

df_clean.head()

"""## Supervised model to predict any new reviews to the relevant Topics.

You have now build the model to create the topics for each review.Now in the below section you will use them to classify any new reviews.

Since you will be using supervised learning technique we have to convert the topic names to numbers(numpy arrays only understand numbers)
"""

#Create the dictionary again of Topic names and Topics

Topic_names = { 'Promotion related reviews' : 0,
                'Good reviews' : 1 ,
                'Dissapointed reviews' : 2 ,
                'loved product' : 3 , 
                'great product' : 4}
#Replace Topics with Topic Names
df_clean['Topic_Number'] = df_clean['Topic'].map(Topic_names)

df_clean.head()

#Keep the columns"reviews_text_cleaned" & "Topic" only in the new dataframe --> training_data
training_data= df_clean[['reviews_text_cleaned', 'Topic_Number']]

training_data.head()

"""#### Apply the supervised models on the training data created. In this process, you have to do the following:
* Create the vector counts using Count Vectoriser
* Transform the word vecotr to tf-idf
* Create the train & test data using the train_test_split on the tf-idf & topics

"""

#Write your code here to transform the word vector to tf-idf

X = tf_fit.transform(training_data['reviews_text_cleaned'])

#Shape of the Predictor Variables
X.shape

#Shape of the Response Variable
Y = training_data['Topic_Number']
Y.shape

"""You have to try atleast 3 models on the train & test data from these options:
* Logistic regression
* Decision Tree
* Random Forest
* Naive Bayes (optional)

**Using the required evaluation metrics judge the tried models and select the ones performing the best**

## APPLYING LOGISTIC REGRESSION

### Since our data pre-processing has been performed, we can proceed with the model building
"""

# install yellowbrick to visualize ROC curve of a multiclass problem
!pip install yellowbrick

# Modeling librarries
from sklearn.linear_model import LogisticRegression
from yellowbrick.classifier import ConfusionMatrix


from sklearn import metrics
from sklearn.model_selection import train_test_split
from yellowbrick.classifier import ROCAUC

# define the multinomial logistic regression model

lr = LogisticRegression(multi_class='ovr', solver='liblinear')

#test-train split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=50)

X_train.shape

#fitting the model on train data

lr_model = lr.fit(X_train, y_train)

# make predictions on train set 

pred_dept_train = lr_model.predict(X_train)

# make predictions on test set 

pred_dept_test = lr_model.predict(X_test)

#printing the performance metrics for train data
print(metrics.classification_report(y_train, pred_dept_train ))

#printing the performance metrics for test data
print(metrics.classification_report(y_test, pred_dept_test))

"""## Observations : Logistic Regression

* Accuracy = 0.94
* Precision = 0.94
* Recall  = 0.94
* F1-score = 0.94

## APPLYING RANDOM-FOREST CLASSIFIER
"""

#importing required libraries

from sklearn.ensemble import RandomForestClassifier

# Initialize a simple Random Forest Classifier

rf = RandomForestClassifier(n_estimators=15, max_depth=20, max_features=30, random_state=100, 
                            oob_score=True)

#creating a copy of test train data for the Random Forest Classifier

X1_train = X_train
Y1_train = y_train
X1_test = X_test
Y1_test = y_test

#fitting a simple random forest classifier on train data

rf.fit(X1_train, Y1_train)

#printing the oob score
rf.oob_score_

#predicting the class labels on the train data
y1_pred_train=rf.predict(X1_train)

#predicting the class labels on the test data

y1_pred_test = rf.predict(X1_test)

#printing the performance metrics for the train data

print(metrics.classification_report(Y1_train, y1_pred_train))

#printing the performance metrics for the test data

print(metrics.classification_report(Y1_test, y1_pred_test))

"""## Observations : Random-Forest Classifier

* Accuracy = 0.69
* Precision = 0.73
* Recall  = 0.69
* F1-score = 0.64

## APPLYING NAIVE BAYES ALGORITHM
"""

#importing the required libraries
from sklearn.naive_bayes import MultinomialNB

#initializing a multinomial-naive-bayes classifier

mnb = MultinomialNB()

#fitting the model on Train Data
mnb.fit(X1_train, Y1_train)

#predicting the class labels for train and test data

ytrain_pred_mnb = mnb.predict(X1_train)
ytest_pred_mnb = mnb.predict(X1_test)

#printing performance metrics for test data

print(metrics.classification_report(Y1_train, ytrain_pred_mnb))

#printing perfomance metrics for test data

print(metrics.classification_report(Y1_test, ytest_pred_mnb))

"""## Observations : Naive Bayes Model

* Accuracy = 0.78
* Precision = 0.78
* Recall  = 0.78
* F1-score = 0.74

## CHOOSING BEST MODEL
"""

#creating the metrics dataframe for all the above run models
metrics = pd.DataFrame({'Model': ['Logistic Regression', 'Random-Forest', 'Naive Bayes'],
                        'Accuracy': [0.94 , 0.69 , 0.78],
                        'Precision': [0.94,0.73 ,0.78],
                        'Recall': [0.94 ,0.69,0.78],
                        'F1-Score': [0.94 ,0.64,0.74]}  )

#printing the overall metrics
metrics

"""**From overall metrics we can see that Logistic Regression performs the best when compared to other models**

## Saving the model
"""

import pickle

def save_object(obj, filename):
    filename = "pickle\\"+filename+'.pkl'
    pickle.dump(obj, open(filename, 'wb'))

save_object(lr, 'sentiment-classification-logistic-model')

save_object(tfidf_vect, 'tfidf-vectorizer')

save_object(df_clean, 'cleaned-data')

"""### Recommendation System
Different Approaches to develop Recommendation System -

- Demographich based Recommendation System

- Content Based Recommendation System

- Collaborative filtering Recommendation System

### Here we are going to use Collaborative filtering Recommendation system
"""

df.info()

df.head()

df_recom = df[["id", "name", "reviews_rating", "reviews_username"]]

# summing up the missing values (column-wise)
round(100*(df.isnull().sum()/len(df.index)), 2)

# Test and Train split of the dataset.
train, test = train_test_split(df_recom, test_size=0.30, random_state=42)

print(train.shape)
print(test.shape)

# Pivot the train ratings' dataset into matrix format in which columns are id and the rows are usernames.
df_pivot = train.pivot_table(
    index='reviews_username',
    columns='id',
    values='reviews_rating'
).fillna(0)

df_pivot.head(10)

"""### Creating dummy train & dummy test dataset
These dataset will be used for prediction 
- Dummy train will be used later for prediction of the products which has not been rated by the user. To ignore the product rated by the user, we will mark it as 0 during prediction. The products not rated by user is marked as 1 for prediction in dummy train dataset. 

- Dummy test will be used for evaluation. To evaluate, we will only make prediction on the products rated by the user. So, this is marked as 1. This is just opposite of dummy_train.
"""

# Copy the train dataset into dummy_train
dummy_train = train.copy()

dummy_train.head()

# The products not rated by user is marked as 1 for prediction. 
dummy_train['reviews_rating'] = dummy_train['reviews_rating'].apply(lambda x: 0 if x>=1 else 1)

# Convert the dummy train dataset into matrix format.
dummy_train = dummy_train.pivot_table(
    index='reviews_username',
    columns='id',
    values='reviews_rating'
).fillna(1)

dummy_train.head()

df_pivot.index.nunique()

"""**Cosine Similarity**

Cosine Similarity is a measurement that quantifies the similarity between two vectors [Which is Rating Vector in this case] 

**Adjusted Cosine**

Adjusted cosine similarity is a modified version of vector-based similarity where we incorporate the fact that different users have different ratings schemes. In other words, some users might rate items highly in general, and others might give items lower ratings as a preference. To handle this nature from rating given by user , we subtract average ratings for each user from each user's rating for different products.

## User Similarity Matrix

## Using Cosine Similarity
"""

from sklearn.metrics.pairwise import pairwise_distances

# Creating the User Similarity Matrix using pairwise_distance function.
user_correlation = 1 - pairwise_distances(df_pivot, metric='cosine')
user_correlation[np.isnan(user_correlation)] = 0
print(user_correlation)

user_correlation.shape

"""## Using adjusted Cosine

### Here, we are not removing the NaN values and calculating the mean only for the products rated by the user
"""

# Create a user-product matrix.
df_pivot = train.pivot_table(
    index='reviews_username',
    columns='id',
    values='reviews_rating'
)

df_pivot.head()

"""### Normalising the rating of the movie for each user around 0 mean"""

mean = np.nanmean(df_pivot, axis=1)
df_subtracted = (df_pivot.T-mean).T

df_subtracted.head()

"""### Finding cosine similarity"""

from sklearn.metrics.pairwise import pairwise_distances

# Creating the User Similarity Matrix using pairwise_distance function.
user_correlation = 1 - pairwise_distances(df_subtracted.fillna(0), metric='cosine')
user_correlation[np.isnan(user_correlation)] = 0
print(user_correlation)

"""## Prediction - User User

Doing the prediction for the users which are positively related with other users, and not the users which are negatively related as we are interested in the users which are more similar to the current users. So, ignoring the correlation for values less than 0.
"""

user_correlation[user_correlation<0]=0
user_correlation

"""Rating predicted by the user"""

user_predicted_ratings = np.dot(user_correlation, df_pivot.fillna(0))
user_predicted_ratings

user_predicted_ratings.shape

user_final_rating = np.multiply(user_predicted_ratings,dummy_train)
user_final_rating.head()

"""### Finding the top 20 recommendation for the *user*"""

# Take the user ID as input. eg : 08dallas
user_input = '08dallas'
print(user_input)

user_final_rating.head(2)

recommendations = user_final_rating.loc[user_input].sort_values(ascending=False)[0:20]
recommendations

#display the top 20 product id, name and similarity_score 
final_recommendations = pd.DataFrame({'product_id': recommendations.index, 'similarity_score' : recommendations})
final_recommendations.reset_index(drop=True)
pd.merge(final_recommendations, train, on="id")[["id", "name", "similarity_score"]].drop_duplicates()

"""# Evaluation - User User

Evaluation will we same as you have seen above for the prediction.
"""

# Find out the common users of test and train dataset.
common = test[test.reviews_username.isin(train.reviews_username)]
common.shape

common.head()

common_user_based_matrix = common.pivot_table(index='reviews_username',columns='id', values='reviews_rating')

# Convert the user_correlation matrix into dataframe.
user_correlation_df = pd.DataFrame(user_correlation)

df_subtracted.head(1)

user_correlation_df['reviews_username'] = df_subtracted.index
user_correlation_df.set_index('reviews_username',inplace=True)
user_correlation_df.head()

common.head(1)

list_name = common.reviews_username.tolist()

user_correlation_df.columns = df_subtracted.index.tolist()


user_correlation_df_1 =  user_correlation_df[user_correlation_df.index.isin(list_name)]

user_correlation_df_1.shape

user_correlation_df_2 = user_correlation_df_1.T[user_correlation_df_1.T.index.isin(list_name)]

user_correlation_df_3 = user_correlation_df_2.T

user_correlation_df_3.head()

user_correlation_df_3.shape

user_correlation_df_3[user_correlation_df_3<0]=0

common_user_predicted_ratings = np.dot(user_correlation_df_3, common_user_based_matrix.fillna(0))
common_user_predicted_ratings

dummy_test = common.copy()

dummy_test['reviews_rating'] = dummy_test['reviews_rating'].apply(lambda x: 1 if x>=1 else 0)

dummy_test = dummy_test.pivot_table(index='reviews_username',columns='id', values='reviews_rating').fillna(0)

dummy_test.shape

common_user_predicted_ratings = np.multiply(common_user_predicted_ratings,dummy_test)

common_user_predicted_ratings.head(2)

"""Calculating the RMSE for only the movies rated by user. For RMSE, normalising the rating to (1,5) range."""

from sklearn.preprocessing import MinMaxScaler
from numpy import *

X  = common_user_predicted_ratings.copy() 
X = X[X>0]

scaler = MinMaxScaler(feature_range=(1, 5))
print(scaler.fit(X))
y = (scaler.transform(X))

print(y)

common_ = common.pivot_table(index='reviews_username', columns='id', values='reviews_rating')

# Finding total non-NaN value
total_non_nan = np.count_nonzero(~np.isnan(y))

rmse = (sum(sum((common_ - y )**2))/total_non_nan)**0.5
print(rmse)

"""## Using Item similarity

# Item Based Similarity

Taking the transpose of the rating matrix to normalize the rating around the mean for different movie ID. In the user based similarity, we had taken mean for each user instead of each movie.
"""

product_column = "id"
user_column = "reviews_username"
value_column = "reviews_rating"

df_pivot = pd.pivot_table(train,
    index=product_column,
    columns=user_column,
    values=value_column
)

df_pivot.head()

mean = np.nanmean(df_pivot, axis=1)
df_subtracted = (df_pivot.T-mean).T

df_subtracted.head()

"""Finding the cosine similarity using pairwise distances approach"""

from sklearn.metrics.pairwise import pairwise_distances

# Item Similarity Matrix
item_correlation = 1 - pairwise_distances(df_subtracted.fillna(0), metric='cosine')
item_correlation[np.isnan(item_correlation)] = 0
print(item_correlation)

"""Filtering the correlation only for which the value is greater than 0. (Positively correlated)"""

item_correlation[item_correlation<0]=0
item_correlation

"""# Prediction - Item Item"""

item_predicted_ratings = np.dot((df_pivot.fillna(0).T),item_correlation)
item_predicted_ratings

item_predicted_ratings.shape

dummy_train.shape

"""### Filtering the rating only for the product not rated by the user for recommendation"""

item_final_rating = np.multiply(item_predicted_ratings,dummy_train)
item_final_rating.head()

"""### Finding the top 20 recommendation for the *user*


"""

# Take the user ID as input eg : 08dallas
user_input = '08dallas'
print(user_input)

item_final_rating.head(2)

# Recommending the Top 5 to the user.
item_recommendations = item_final_rating.loc[user_input].sort_values(ascending=False)[0:20]
item_recommendations

#display the top 20  id, name and similarity_score 
item_final_recommendations = pd.DataFrame({'product_id': item_recommendations.index, 'similarity_score' : item_recommendations})
item_final_recommendations.reset_index(drop=True)
pd.merge(item_final_recommendations, train, on="id")[["id", "name", "similarity_score"]].drop_duplicates()

"""# Evaluation - Item Item

Evaluation will we same as you have seen above for the prediction.
"""

test.columns

common = test[test.id.isin(train.id)]
common.shape

common.head(4)

common_item_based_matrix = common.pivot_table(index='id',columns='reviews_username', values='reviews_rating')

common_item_based_matrix.shape

item_correlation_df = pd.DataFrame(item_correlation)

item_correlation_df.head(1)

item_correlation_df['id'] = df_subtracted.index
item_correlation_df.set_index('id',inplace=True)
item_correlation_df.head()

list_name = common.id.tolist()

item_correlation_df.columns = df_subtracted.index.tolist()

item_correlation_df_1 =  item_correlation_df[item_correlation_df.index.isin(list_name)]

item_correlation_df_2 = item_correlation_df_1.T[item_correlation_df_1.T.index.isin(list_name)]

item_correlation_df_3 = item_correlation_df_2.T

item_correlation_df_3.head(10)

df_subtracted

item_correlation_df_3[item_correlation_df_3<0]=0

common_item_predicted_ratings = np.dot(item_correlation_df_3, common_item_based_matrix.fillna(0))
common_item_predicted_ratings

common_item_predicted_ratings.shape

"""Dummy test will be used for evaluation. To evaluate, we will only make prediction on the ones rated by the user. So, this is marked as 1. This is just opposite of dummy_train


"""

dummy_test = common.copy()

dummy_test['reviews_rating'] = dummy_test['reviews_rating'].apply(lambda x: 1 if x>=1 else 0)

dummy_test = dummy_test.pivot_table(index='id',columns='reviews_username', values='reviews_rating').fillna(0)

common_item_predicted_ratings = np.multiply(common_item_predicted_ratings,dummy_test)

"""The products not rated is marked as 0 for evaluation. And make the item- item matrix representaion.

"""

common_ = common.pivot_table(index='id',columns='reviews_username', values='reviews_rating')

from sklearn.preprocessing import MinMaxScaler
from numpy import *

X  = common_item_predicted_ratings.copy() 
X = X[X>0]

scaler = MinMaxScaler(feature_range=(1, 5))
print(scaler.fit(X))
y = (scaler.transform(X))

print(y)

# Finding total non-NaN value
total_non_nan = np.count_nonzero(~np.isnan(y))

rmse = (sum(sum((common_ - y )**2))/total_non_nan)**0.5
print(rmse)

"""On comparing the RMSE values of User Based Recommender and Item Based Recommender, User based recommendation model seems to be better in this case, as it has a lower RMSE value (~2)"""

# saving the correlation matrix of user based recommender 
save_object(user_final_rating, "user_final_rating")

"""## Top Product Recommendations - Recommendation of 20 products and filtering by sentiment model
Get the top 20 product recommendations using the recommender system and get the top 5 using the sentiment ML model.


"""

def get_sentiment_recommendations(user):
    if (user in user_final_rating.index):
        # get the product recommedation using the trained ML model
        recommendations = list(user_final_rating.loc[user].sort_values(ascending=False)[0:20].index)
        temp = df_clean[df_clean.id.isin(recommendations)]
        X =  tfidf_vect.transform(temp["reviews_text_cleaned"].values.astype(str))
        temp["predicted_sentiment"]= lr.predict(X)
        temp = temp[['name','predicted_sentiment']]
        temp_grouped = temp.groupby('name', as_index=False).count()
        temp_grouped["pos_review_count"] = temp_grouped.name.apply(lambda x: temp[(temp.name==x) & (temp.predicted_sentiment==1)]["predicted_sentiment"].count())
        temp_grouped["total_review_count"] = temp_grouped['predicted_sentiment']
        temp_grouped['pos_sentiment_percent'] = np.round(temp_grouped["pos_review_count"]/temp_grouped["total_review_count"]*100,2)
        return temp_grouped.sort_values('pos_sentiment_percent', ascending=False)
    else:
        print(f"User name {user} doesn't exist")

#testing the above fuction using one of the users that's trained on.
get_sentiment_recommendations("08dallas")

#get the top 5
get_sentiment_recommendations("08dallas")[:5]

#testing the above fuction on the user that doesn't exists or a new user
get_sentiment_recommendations("test123")

# Save the final dataframe to a csv files 
# so that we can use this dataframe in our flask app
df.to_csv("ratings_df.csv",index=False)